{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import cuda, float32\n",
    "import math\n",
    "import cupy as cp\n",
    "\n",
    "# Check if CUDA is available\n",
    "try:\n",
    "    cuda_device = cuda.get_current_device()\n",
    "    print(f\"CUDA Device: {cuda_device.name}\")\n",
    "    print(f\"Compute Capability: {cuda_device.compute_capability}\")\n",
    "    print(f\"Memory: {cuda_device.total_memory / 1e9:.2f} GB\")\n",
    "    has_gpu = True\n",
    "except:\n",
    "    print(\"No CUDA device detected\")\n",
    "    has_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrices(n, dtype=np.float32):\n",
    "    \"\"\"Generate two random square matrices of size n x n\"\"\"\n",
    "    A = np.random.random((n, n)).astype(dtype)\n",
    "    B = np.random.random((n, n)).astype(dtype)\n",
    "    return A, B\n",
    "\n",
    "def validate_result(C_reference, C_test, rtol=1e-5):\n",
    "    \"\"\"Check if the calculated result matches the reference\"\"\"\n",
    "    return np.allclose(C_reference, C_test, rtol=rtol)\n",
    "\n",
    "def benchmark(func, sizes, repeat=3):\n",
    "    \"\"\"Benchmark a function for different matrix sizes\"\"\"\n",
    "    times = []\n",
    "    for n in sizes:\n",
    "        A, B = generate_matrices(n)\n",
    "        \n",
    "        # Warm-up run\n",
    "        _ = func(A, B)\n",
    "        \n",
    "        # Timed runs\n",
    "        elapsed = []\n",
    "        for _ in range(repeat):\n",
    "            start = time.time()\n",
    "            _ = func(A, B)\n",
    "            # Make sure CUDA operations are completed\n",
    "            if has_gpu:\n",
    "                cuda.synchronize()\n",
    "            elapsed.append(time.time() - start)\n",
    "        \n",
    "        times.append(min(elapsed))  # Use minimum time for best performance\n",
    "        print(f\"Size {n}x{n}: {min(elapsed):.5f} seconds\")\n",
    "    \n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive CPU based Matrix Multiplication\n",
    "\n",
    "def naive_cpu_matmul(A, B):\n",
    "    #Basic CPU matrix multiplication with triple nested loops\n",
    "    n = A.shape[0]  # Assuming square matrices\n",
    "    C = np.zeros((n, n), dtype=A.dtype)\n",
    "    \n",
    "    # Triple nested loop - the classic O(nÂ³) algorithm\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(n):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "                \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_matmul(A, B):\n",
    "    #NumPy's optimized matrix multiplication\n",
    "    return np.matmul(A, B)\n",
    "\n",
    "# Test and verify our naive implementation\n",
    "n_test = 100  # Small enough for a quick check\n",
    "A_test, B_test = generate_matrices(n_test)\n",
    "\n",
    "# Calculate reference result using NumPy\n",
    "C_numpy = numpy_matmul(A_test, B_test)\n",
    "\n",
    "# Calculate result using our naive implementation\n",
    "C_naive = naive_cpu_matmul(A_test, B_test)\n",
    "\n",
    "# Verify the result\n",
    "print(f\"Naive CPU implementation correct: {validate_result(C_numpy, C_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU-accelerated Matrix Multiplication using Numba\n",
    "@cuda.jit\n",
    "def cuda_matmul_kernel(A, B, C):\n",
    "    #CUDA kernel for matrix multiplication.\n",
    "    #Each thread computes one element of C.\n",
    "\n",
    "    # Get thread indices\n",
    "    i, j = cuda.grid(2)\n",
    "    \n",
    "    # Check if thread is within valid range\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        # Compute C[i,j] = sum(A[i,:] * B[:,j])\n",
    "        tmp = 0.0\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp\n",
    "\n",
    "def numba_cuda_matmul(A, B):\n",
    "    #Perform matrix multiplication on GPU using Numba CUDA\n",
    "\n",
    "    # Make sure we're working with the right data type\n",
    "    A = np.asarray(A, dtype=np.float32)\n",
    "    B = np.asarray(B, dtype=np.float32)\n",
    "    \n",
    "    # Initialize result matrix\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    # Copy matrices to device\n",
    "    A_device = cuda.to_device(A)\n",
    "    B_device = cuda.to_device(B)\n",
    "    C_device = cuda.to_device(C)\n",
    "    \n",
    "    # Set up grid dimensions\n",
    "    # You might need to adjust these for your specific GPU\n",
    "    threads_per_block = (16, 16)\n",
    "    blocks_per_grid_x = math.ceil(A.shape[0] / threads_per_block[0])\n",
    "    blocks_per_grid_y = math.ceil(B.shape[1] / threads_per_block[1])\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "    \n",
    "    # Launch kernel\n",
    "    cuda_matmul_kernel[blocks_per_grid, threads_per_block](A_device, B_device, C_device)\n",
    "    \n",
    "    # Retrieve result from device\n",
    "    C = C_device.copy_to_host()\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_gpu:\n",
    "    # Test GPU implementation on small matrices\n",
    "    C_numba = numba_cuda_matmul(A_test, B_test)\n",
    "    print(f\"Numba CUDA implementation correct: {validate_result(C_numpy, C_numba)}\")\n",
    "else:\n",
    "    print(\"Skipping CUDA test (no GPU available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuBLAS based Optimized Matrix Multiplication\n",
    "def cublas_matmul(A, B):\n",
    "    #Matrix multiplication using cuBLAS (via CuPy)\n",
    "\n",
    "    # Convert to CuPy arrays\n",
    "    A_gpu = cp.asarray(A)\n",
    "    B_gpu = cp.asarray(B)\n",
    "    \n",
    "    # Compute using cuBLAS\n",
    "    C_gpu = cp.matmul(A_gpu, B_gpu)\n",
    "    \n",
    "    # Copy result back to CPU\n",
    "    C = cp.asnumpy(C_gpu)\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_gpu:\n",
    "    # Test cuBLAS implementation\n",
    "    C_cublas = cublas_matmul(A_test, B_test)\n",
    "    print(f\"cuBLAS implementation correct: {validate_result(C_numpy, C_cublas)}\")\n",
    "else:\n",
    "    print(\"Skipping cuBLAS test (no GPU available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def tiled_matmul_kernel(A, B, C): \n",
    "    #CUDA kernel for matrix multiplication using shared memory tiles\n",
    "    \n",
    "    # Define shared memory arrays for the tiles\n",
    "    TILE_SIZE = 16  # Tile size (must match threads_per_block)\n",
    "    shared_A = cuda.shared.array(shape=(TILE_SIZE, TILE_SIZE), dtype=float32)\n",
    "    shared_B = cuda.shared.array(shape=(TILE_SIZE, TILE_SIZE), dtype=float32)\n",
    "    \n",
    "    # Thread and block indices\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    \n",
    "    # Row and column in the result matrix C\n",
    "    row = by * TILE_SIZE + ty\n",
    "    col = bx * TILE_SIZE + tx\n",
    "    \n",
    "    # Accumulate result for C[row, col]\n",
    "    tmp = 0.0\n",
    "    \n",
    "    # Loop over tiles of A and B\n",
    "    for t in range((A.shape[1] + TILE_SIZE - 1) // TILE_SIZE):\n",
    "        # Load A tile into shared memory\n",
    "        if row < A.shape[0] and t * TILE_SIZE + tx < A.shape[1]:\n",
    "            shared_A[ty, tx] = A[row, t * TILE_SIZE + tx]\n",
    "        else:\n",
    "            shared_A[ty, tx] = 0.0\n",
    "            \n",
    "        # Load B tile into shared memory\n",
    "        if t * TILE_SIZE + ty < B.shape[0] and col < B.shape[1]:\n",
    "            shared_B[ty, tx] = B[t * TILE_SIZE + ty, col]\n",
    "        else:\n",
    "            shared_B[ty, tx] = 0.0\n",
    "            \n",
    "        # Wait until all threads load their tiles\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        # Compute dot product for the current tile\n",
    "        for k in range(TILE_SIZE):\n",
    "            tmp += shared_A[ty, k] * shared_B[k, tx]\n",
    "            \n",
    "        # Ensure all computations are done before loading next tile\n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    # Write final result to C\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        C[row, col] = tmp\n",
    "\n",
    "def tiled_shared_matmul(A, B):\n",
    "    #Perform matrix multiplication on GPU using shared memory tiles\n",
    "\n",
    "    # Make sure we're working with the right data type\n",
    "    A = np.asarray(A, dtype=np.float32)\n",
    "    B = np.asarray(B, dtype=np.float32)\n",
    "    \n",
    "    # Initialize result matrix\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    # Copy matrices to device\n",
    "    A_device = cuda.to_device(A)\n",
    "    B_device = cuda.to_device(B)\n",
    "    C_device = cuda.to_device(C)\n",
    "    \n",
    "    # Set up grid dimensions\n",
    "    TILE_SIZE = 16\n",
    "    threads_per_block = (TILE_SIZE, TILE_SIZE)\n",
    "    blocks_per_grid_x = math.ceil(B.shape[1] / TILE_SIZE)\n",
    "    blocks_per_grid_y = math.ceil(A.shape[0] / TILE_SIZE)\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "    \n",
    "    # Launch kernel\n",
    "    tiled_matmul_kernel[blocks_per_grid, threads_per_block](A_device, B_device, C_device)\n",
    "    \n",
    "    # Retrieve result from device\n",
    "    C = C_device.copy_to_host()\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_gpu:\n",
    "    # Test tiled shared memory implementation\n",
    "    C_tiled = tiled_shared_matmul(A_test, B_test)\n",
    "    print(f\"Tiled shared memory implementation correct: {validate_result(C_numpy, C_tiled)}\")\n",
    "else:\n",
    "    print(\"Skipping tiled shared memory test (no GPU available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpi_matmul_example(): \n",
    "    #Example code showing how multi-GPU matrix multiplication would be implemented.\n",
    "    \n",
    "    #This code would need to be saved as a separate file and run with:\n",
    "    #$ mpirun -np <num_gpus> python mpi_matmul.py\n",
    "\n",
    "    try:\n",
    "        from mpi4py import MPI\n",
    "    except ImportError:\n",
    "        print(\"mpi4py not installed. Please install it for multi-GPU support.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize MPI\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()  # Current process ID\n",
    "    size = comm.Get_size()  # Total number of processes\n",
    "    \n",
    "    # Select GPU based on rank\n",
    "    if has_gpu:\n",
    "        device_id = rank % cuda.get_num_devices()\n",
    "        cp.cuda.Device(device_id).use()\n",
    "        print(f\"Process {rank} using GPU {device_id}\")\n",
    "    \n",
    "    # Matrix size\n",
    "    n = 1000\n",
    "    \n",
    "    # Process 0 generates the matrices\n",
    "    if rank == 0:\n",
    "        print(f\"Generating {n}x{n} matrices\")\n",
    "        A, B = generate_matrices(n)\n",
    "        \n",
    "        # Compute reference solution for validation\n",
    "        C_ref = numpy_matmul(A, B)\n",
    "    else:\n",
    "        A = None\n",
    "        B = None\n",
    "        C_ref = None\n",
    "    \n",
    "    # Broadcast matrix B to all processes (all processes need the entire B)\n",
    "    B = comm.bcast(B, root=0)\n",
    "    \n",
    "    # Scatter rows of A among processes\n",
    "    rows_per_proc = n // size\n",
    "    A_local = np.zeros((rows_per_proc, n), dtype=np.float32)\n",
    "    comm.Scatter([A, MPI.FLOAT], [A_local, MPI.FLOAT], root=0)\n",
    "    \n",
    "    # Each process computes its part using CuPy/cuBLAS\n",
    "    if has_gpu:\n",
    "        A_local_gpu = cp.asarray(A_local)\n",
    "        B_gpu = cp.asarray(B)\n",
    "        C_local_gpu = cp.matmul(A_local_gpu, B_gpu)\n",
    "        C_local = cp.asnumpy(C_local_gpu)\n",
    "    else:\n",
    "        C_local = np.matmul(A_local, B)\n",
    "    \n",
    "    # Gather results\n",
    "    C = None\n",
    "    if rank == 0:\n",
    "        C = np.zeros((n, n), dtype=np.float32)\n",
    "    \n",
    "    comm.Gather([C_local, MPI.FLOAT], [C, MPI.FLOAT], root=0)\n",
    "    \n",
    "    # Validate result on process 0\n",
    "    if rank == 0:\n",
    "        print(f\"Multi-GPU result correct: {validate_result(C_ref, C)}\")\n",
    "        \n",
    "print(\"Multi-GPU implementation would need to be run separately with mpirun.\")\n",
    "print(\"Above is a sample implementation sketch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_gpu:\n",
    "    # Define sizes to benchmark (increase these for more meaningful comparisons)\n",
    "    sizes = [128, 256, 512, 1024]\n",
    "    \n",
    "    # Skip naive CPU for larger sizes as it's too slow\n",
    "    cpu_sizes = [128, 256, 512]\n",
    "    \n",
    "    print(\"\\nBenchmarking naive CPU implementation:\")\n",
    "    cpu_times = benchmark(naive_cpu_matmul, cpu_sizes)\n",
    "    \n",
    "    print(\"\\nBenchmarking NumPy implementation:\")\n",
    "    numpy_times = benchmark(numpy_matmul, sizes)\n",
    "    \n",
    "    print(\"\\nBenchmarking Numba CUDA implementation:\")\n",
    "    numba_times = benchmark(numba_cuda_matmul, sizes)\n",
    "    \n",
    "    print(\"\\nBenchmarking cuBLAS implementation:\")\n",
    "    cublas_times = benchmark(cublas_matmul, sizes)\n",
    "    \n",
    "    print(\"\\nBenchmarking tiled shared memory implementation:\")\n",
    "    tiled_times = benchmark(tiled_shared_matmul, sizes)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(cpu_sizes, cpu_times, marker='o', label='Naive CPU')\n",
    "    plt.plot(sizes, numpy_times, marker='s', label='NumPy')\n",
    "    plt.plot(sizes, numba_times, marker='^', label='Numba CUDA')\n",
    "    plt.plot(sizes, cublas_times, marker='d', label='cuBLAS')\n",
    "    plt.plot(sizes, tiled_times, marker='*', label='Tiled Shared Memory')\n",
    "    \n",
    "    plt.title('Matrix Multiplication Performance')\n",
    "    plt.xlabel('Matrix Size (n x n)')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.xscale('log2')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate speedup relative to NumPy\n",
    "    print(\"\\nSpeedup relative to NumPy:\")\n",
    "    for i, size in enumerate(sizes):\n",
    "        if i < len(cpu_sizes):\n",
    "            print(f\"Size {size}x{size}:\")\n",
    "            print(f\"  Naive CPU: {numpy_times[i]/cpu_times[i]:.2f}x slower\")\n",
    "        print(f\"  Numba CUDA: {numpy_times[i]/numba_times[i]:.2f}x faster\")\n",
    "        print(f\"  cuBLAS: {numpy_times[i]/cublas_times[i]:.2f}x faster\")\n",
    "        print(f\"  Tiled Shared Memory: {numpy_times[i]/tiled_times[i]:.2f}x faster\")\n",
    "else:\n",
    "    print(\"GPU benchmarks skipped (no GPU available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMatrix Multiplication Method Comparison:\")\n",
    "print(\"1. Naive CPU: Simple triple nested loop implementation. Very slow for large matrices.\")\n",
    "print(\"2. Numba CUDA: Basic GPU implementation. Good improvement over CPU.\")\n",
    "print(\"3. cuBLAS: Highly optimized library implementation. Usually the fastest method.\")\n",
    "print(\"4. Tiled Shared Memory: Improved GPU implementation with better memory access patterns.\")\n",
    "print(\"5. Multi-GPU: Distributes work across multiple GPUs. Best for very large matrices.\")\n",
    "print(\"\\nKey findings:\")\n",
    "if has_gpu:\n",
    "    fastest = \"cuBLAS\"\n",
    "    print(f\"- The fastest method for most cases is {fastest}\")\n",
    "    print(f\"- GPU implementations show {numpy_times[-1]/cublas_times[-1]:.1f}x speedup over NumPy for {sizes[-1]}x{sizes[-1]} matrices\")\n",
    "    print(\"- Memory optimization with tiling provides significant benefit for larger matrices\")\n",
    "    print(\"- Multi-GPU scaling would be beneficial for matrices too large for a single GPU's memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
