{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import cuda, float32\n",
    "import math\n",
    "import cupy as cp\n",
    "\n",
    "# Check if CUDA is available\n",
    "try:\n",
    "    cuda_device = cuda.get_current_device()\n",
    "    print(f\"CUDA Device: {cuda_device.name}\")\n",
    "    print(f\"Compute Capability: {cuda_device.compute_capability}\")\n",
    "    print(f\"Memory: {cuda_device.total_memory / 1e9:.2f} GB\")\n",
    "    has_gpu = True\n",
    "except:\n",
    "    print(\"No CUDA device detected\")\n",
    "    has_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrices(n, dtype=np.float32):\n",
    "    \"\"\"Generate two random square matrices of size n x n\"\"\"\n",
    "    A = np.random.random((n, n)).astype(dtype)\n",
    "    B = np.random.random((n, n)).astype(dtype)\n",
    "    return A, B\n",
    "\n",
    "def validate_result(C_reference, C_test, rtol=1e-5):\n",
    "    \"\"\"Check if the calculated result matches the reference\"\"\"\n",
    "    return np.allclose(C_reference, C_test, rtol=rtol)\n",
    "\n",
    "def benchmark(func, sizes, repeat=3):\n",
    "    \"\"\"Benchmark a function for different matrix sizes\"\"\"\n",
    "    times = []\n",
    "    for n in sizes:\n",
    "        A, B = generate_matrices(n)\n",
    "        \n",
    "        # Warm-up run\n",
    "        _ = func(A, B)\n",
    "        \n",
    "        # Timed runs\n",
    "        elapsed = []\n",
    "        for _ in range(repeat):\n",
    "            start = time.time()\n",
    "            _ = func(A, B)\n",
    "            # Make sure CUDA operations are completed\n",
    "            if has_gpu:\n",
    "                cuda.synchronize()\n",
    "            elapsed.append(time.time() - start)\n",
    "        \n",
    "        times.append(min(elapsed))  # Use minimum time for best performance\n",
    "        print(f\"Size {n}x{n}: {min(elapsed):.5f} seconds\")\n",
    "    \n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive CPU based Matrix Multiplication\n",
    "\n",
    "def naive_cpu_matmul(A, B):\n",
    "    #Basic CPU matrix multiplication with triple nested loops\n",
    "    n = A.shape[0]  # Assuming square matrices\n",
    "    C = np.zeros((n, n), dtype=A.dtype)\n",
    "    \n",
    "    # Triple nested loop - the classic O(nÂ³) algorithm\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            for k in range(n):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "                \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_matmul(A, B):\n",
    "    #NumPy's optimized matrix multiplication\n",
    "    return np.matmul(A, B)\n",
    "\n",
    "# Test and verify our naive implementation\n",
    "n_test = 100  # Small enough for a quick check\n",
    "A_test, B_test = generate_matrices(n_test)\n",
    "\n",
    "# Calculate reference result using NumPy\n",
    "C_numpy = numpy_matmul(A_test, B_test)\n",
    "\n",
    "# Calculate result using our naive implementation\n",
    "C_naive = naive_cpu_matmul(A_test, B_test)\n",
    "\n",
    "# Verify the result\n",
    "print(f\"Naive CPU implementation correct: {validate_result(C_numpy, C_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU-accelerated Matrix Multiplication using Numba\n",
    "@cuda.jit\n",
    "def cuda_matmul_kernel(A, B, C):\n",
    "    #CUDA kernel for matrix multiplication.\n",
    "    #Each thread computes one element of C.\n",
    "\n",
    "    # Get thread indices\n",
    "    i, j = cuda.grid(2)\n",
    "    \n",
    "    # Check if thread is within valid range\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        # Compute C[i,j] = sum(A[i,:] * B[:,j])\n",
    "        tmp = 0.0\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp\n",
    "\n",
    "def numba_cuda_matmul(A, B):\n",
    "    #Perform matrix multiplication on GPU using Numba CUDA\n",
    "\n",
    "    # Make sure we're working with the right data type\n",
    "    A = np.asarray(A, dtype=np.float32)\n",
    "    B = np.asarray(B, dtype=np.float32)\n",
    "    \n",
    "    # Initialize result matrix\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    # Copy matrices to device\n",
    "    A_device = cuda.to_device(A)\n",
    "    B_device = cuda.to_device(B)\n",
    "    C_device = cuda.to_device(C)\n",
    "    \n",
    "    # Set up grid dimensions\n",
    "    # You might need to adjust these for your specific GPU\n",
    "    threads_per_block = (16, 16)\n",
    "    blocks_per_grid_x = math.ceil(A.shape[0] / threads_per_block[0])\n",
    "    blocks_per_grid_y = math.ceil(B.shape[1] / threads_per_block[1])\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "    \n",
    "    # Launch kernel\n",
    "    cuda_matmul_kernel[blocks_per_grid, threads_per_block](A_device, B_device, C_device)\n",
    "    \n",
    "    # Retrieve result from device\n",
    "    C = C_device.copy_to_host()\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_gpu:\n",
    "    # Test GPU implementation on small matrices\n",
    "    C_numba = numba_cuda_matmul(A_test, B_test)\n",
    "    print(f\"Numba CUDA implementation correct: {validate_result(C_numpy, C_numba)}\")\n",
    "else:\n",
    "    print(\"Skipping CUDA test (no GPU available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuBLAS based Optimized Matrix Multiplication\n",
    "def cublas_matmul(A, B):\n",
    "    #Matrix multiplication using cuBLAS (via CuPy)\n",
    "\n",
    "    # Convert to CuPy arrays\n",
    "    A_gpu = cp.asarray(A)\n",
    "    B_gpu = cp.asarray(B)\n",
    "    \n",
    "    # Compute using cuBLAS\n",
    "    C_gpu = cp.matmul(A_gpu, B_gpu)\n",
    "    \n",
    "    # Copy result back to CPU\n",
    "    C = cp.asnumpy(C_gpu)\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_gpu:\n",
    "    # Test cuBLAS implementation\n",
    "    C_cublas = cublas_matmul(A_test, B_test)\n",
    "    print(f\"cuBLAS implementation correct: {validate_result(C_numpy, C_cublas)}\")\n",
    "else:\n",
    "    print(\"Skipping cuBLAS test (no GPU available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def tiled_matmul_kernel(A, B, C): \n",
    "    #CUDA kernel for matrix multiplication using shared memory tiles\n",
    "    \n",
    "    # Define shared memory arrays for the tiles\n",
    "    TILE_SIZE = 16  # Tile size (must match threads_per_block)\n",
    "    shared_A = cuda.shared.array(shape=(TILE_SIZE, TILE_SIZE), dtype=float32)\n",
    "    shared_B = cuda.shared.array(shape=(TILE_SIZE, TILE_SIZE), dtype=float32)\n",
    "    \n",
    "    # Thread and block indices\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    \n",
    "    # Row and column in the result matrix C\n",
    "    row = by * TILE_SIZE + ty\n",
    "    col = bx * TILE_SIZE + tx\n",
    "    \n",
    "    # Accumulate result for C[row, col]\n",
    "    tmp = 0.0\n",
    "    \n",
    "    # Loop over tiles of A and B\n",
    "    for t in range((A.shape[1] + TILE_SIZE - 1) // TILE_SIZE):\n",
    "        # Load A tile into shared memory\n",
    "        if row < A.shape[0] and t * TILE_SIZE + tx < A.shape[1]:\n",
    "            shared_A[ty, tx] = A[row, t * TILE_SIZE + tx]\n",
    "        else:\n",
    "            shared_A[ty, tx] = 0.0\n",
    "            \n",
    "        # Load B tile into shared memory\n",
    "        if t * TILE_SIZE + ty < B.shape[0] and col < B.shape[1]:\n",
    "            shared_B[ty, tx] = B[t * TILE_SIZE + ty, col]\n",
    "        else:\n",
    "            shared_B[ty, tx] = 0.0\n",
    "            \n",
    "        # Wait until all threads load their tiles\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        # Compute dot product for the current tile\n",
    "        for k in range(TILE_SIZE):\n",
    "            tmp += shared_A[ty, k] * shared_B[k, tx]\n",
    "            \n",
    "        # Ensure all computations are done before loading next tile\n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    # Write final result to C\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        C[row, col] = tmp\n",
    "\n",
    "def tiled_shared_matmul(A, B):\n",
    "    #Perform matrix multiplication on GPU using shared memory tiles\n",
    "\n",
    "    # Make sure we're working with the right data type\n",
    "    A = np.asarray(A, dtype=np.float32)\n",
    "    B = np.asarray(B, dtype=np.float32)\n",
    "    \n",
    "    # Initialize result matrix\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    # Copy matrices to device\n",
    "    A_device = cuda.to_device(A)\n",
    "    B_device = cuda.to_device(B)\n",
    "    C_device = cuda.to_device(C)\n",
    "    \n",
    "    # Set up grid dimensions\n",
    "    TILE_SIZE = 16\n",
    "    threads_per_block = (TILE_SIZE, TILE_SIZE)\n",
    "    blocks_per_grid_x = math.ceil(B.shape[1] / TILE_SIZE)\n",
    "    blocks_per_grid_y = math.ceil(A.shape[0] / TILE_SIZE)\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "    \n",
    "    # Launch kernel\n",
    "    tiled_matmul_kernel[blocks_per_grid, threads_per_block](A_device, B_device, C_device)\n",
    "    \n",
    "    # Retrieve result from device\n",
    "    C = C_device.copy_to_host()\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_gpu:\n",
    "    # Test tiled shared memory implementation\n",
    "    C_tiled = tiled_shared_matmul(A_test, B_test)\n",
    "    print(f\"Tiled shared memory implementation correct: {validate_result(C_numpy, C_tiled)}\")\n",
    "else:\n",
    "    print(\"Skipping tiled shared memory test (no GPU available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpi_matmul_example(): \n",
    "    #Example code showing how multi-GPU matrix multiplication would be implemented.\n",
    "    \n",
    "    #This code would need to be saved as a separate file and run with:\n",
    "    #$ mpirun -np <num_gpus> python mpi_matmul.py\n",
    "\n",
    "    try:\n",
    "        from mpi4py import MPI\n",
    "    except ImportError:\n",
    "        print(\"mpi4py not installed. Please install it for multi-GPU support.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize MPI\n",
    "    comm = MPI.COMM_WORLD\n",
    "    rank = comm.Get_rank()  # Current process ID\n",
    "    size = comm.Get_size()  # Total number of processes\n",
    "    \n",
    "    # Select GPU based on rank\n",
    "    if has_gpu:\n",
    "        device_id = rank % cuda.get_num_devices()\n",
    "        cp.cuda.Device(device_id).use()\n",
    "        print(f\"Process {rank} using GPU {device_id}\")\n",
    "    \n",
    "    # Matrix size\n",
    "    n = 1000\n",
    "    \n",
    "    # Process 0 generates the matrices\n",
    "    if rank == 0:\n",
    "        print(f\"Generating {n}x{n} matrices\")\n",
    "        A, B = generate_matrices(n)\n",
    "        \n",
    "        # Compute reference solution for validation\n",
    "        C_ref = numpy_matmul(A, B)\n",
    "    else:\n",
    "        A = None\n",
    "        B = None\n",
    "        C_ref = None\n",
    "    \n",
    "    # Broadcast matrix B to all processes (all processes need the entire B)\n",
    "    B = comm.bcast(B, root=0)\n",
    "    \n",
    "    # Scatter rows of A among processes\n",
    "    rows_per_proc = n // size\n",
    "    A_local = np.zeros((rows_per_proc, n), dtype=np.float32)\n",
    "    comm.Scatter([A, MPI.FLOAT], [A_local, MPI.FLOAT], root=0)\n",
    "    \n",
    "    # Each process computes its part using CuPy/cuBLAS\n",
    "    if has_gpu:\n",
    "        A_local_gpu = cp.asarray(A_local)\n",
    "        B_gpu = cp.asarray(B)\n",
    "        C_local_gpu = cp.matmul(A_local_gpu, B_gpu)\n",
    "        C_local = cp.asnumpy(C_local_gpu)\n",
    "    else:\n",
    "        C_local = np.matmul(A_local, B)\n",
    "    \n",
    "    # Gather results\n",
    "    C = None\n",
    "    if rank == 0:\n",
    "        C = np.zeros((n, n), dtype=np.float32)\n",
    "    \n",
    "    comm.Gather([C_local, MPI.FLOAT], [C, MPI.FLOAT], root=0)\n",
    "    \n",
    "    # Validate result on process 0\n",
    "    if rank == 0:\n",
    "        print(f\"Multi-GPU result correct: {validate_result(C_ref, C)}\")\n",
    "        \n",
    "print(\"Multi-GPU implementation would need to be run separately with mpirun.\")\n",
    "print(\"Above is a sample implementation sketch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_gpu:\n",
    "    # Define sizes to benchmark (increase these for more meaningful comparisons)\n",
    "    sizes = [128, 256, 512, 1024]\n",
    "    \n",
    "    # Skip naive CPU for larger sizes as it's too slow\n",
    "    cpu_sizes = [128, 256, 512]\n",
    "    \n",
    "    print(\"\\nBenchmarking naive CPU implementation:\")\n",
    "    cpu_times = benchmark(naive_cpu_matmul, cpu_sizes)\n",
    "    \n",
    "    print(\"\\nBenchmarking NumPy implementation:\")\n",
    "    numpy_times = benchmark(numpy_matmul, sizes)\n",
    "    \n",
    "    print(\"\\nBenchmarking Numba CUDA implementation:\")\n",
    "    numba_times = benchmark(numba_cuda_matmul, sizes)\n",
    "    \n",
    "    print(\"\\nBenchmarking cuBLAS implementation:\")\n",
    "    cublas_times = benchmark(cublas_matmul, sizes)\n",
    "    \n",
    "    print(\"\\nBenchmarking tiled shared memory implementation:\")\n",
    "    tiled_times = benchmark(tiled_shared_matmul, sizes)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(cpu_sizes, cpu_times, marker='o', label='Naive CPU')\n",
    "    plt.plot(sizes, numpy_times, marker='s', label='NumPy')\n",
    "    plt.plot(sizes, numba_times, marker='^', label='Numba CUDA')\n",
    "    plt.plot(sizes, cublas_times, marker='d', label='cuBLAS')\n",
    "    plt.plot(sizes, tiled_times, marker='*', label='Tiled Shared Memory')\n",
    "    \n",
    "    plt.title('Matrix Multiplication Performance')\n",
    "    plt.xlabel('Matrix Size (n x n)')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.xscale('log2')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate speedup relative to NumPy\n",
    "    print(\"\\nSpeedup relative to NumPy:\")\n",
    "    for i, size in enumerate(sizes):\n",
    "        if i < len(cpu_sizes):\n",
    "            print(f\"Size {size}x{size}:\")\n",
    "            print(f\"  Naive CPU: {numpy_times[i]/cpu_times[i]:.2f}x slower\")\n",
    "        print(f\"  Numba CUDA: {numpy_times[i]/numba_times[i]:.2f}x faster\")\n",
    "        print(f\"  cuBLAS: {numpy_times[i]/cublas_times[i]:.2f}x faster\")\n",
    "        print(f\"  Tiled Shared Memory: {numpy_times[i]/tiled_times[i]:.2f}x faster\")\n",
    "else:\n",
    "    print(\"GPU benchmarks skipped (no GPU available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMatrix Multiplication Method Comparison:\")\n",
    "print(\"1. Naive CPU: Simple triple nested loop implementation. Very slow for large matrices.\")\n",
    "print(\"2. Numba CUDA: Basic GPU implementation. Good improvement over CPU.\")\n",
    "print(\"3. cuBLAS: Highly optimized library implementation. Usually the fastest method.\")\n",
    "print(\"4. Tiled Shared Memory: Improved GPU implementation with better memory access patterns.\")\n",
    "print(\"5. Multi-GPU: Distributes work across multiple GPUs. Best for very large matrices.\")\n",
    "print(\"\\nKey findings:\")\n",
    "if has_gpu:\n",
    "    fastest = \"cuBLAS\"\n",
    "    print(f\"- The fastest method for most cases is {fastest}\")\n",
    "    print(f\"- GPU implementations show {numpy_times[-1]/cublas_times[-1]:.1f}x speedup over NumPy for {sizes[-1]}x{sizes[-1]} matrices\")\n",
    "    print(\"- Memory optimization with tiling provides significant benefit for larger matrices\")\n",
    "    print(\"- Multi-GPU scaling would be beneficial for matrices too large for a single GPU's memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuda\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mQKVAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import cupy as cp\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(QKVAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # QKV projections\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.scale = 1.0 / math.sqrt(self.d_k)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Project input to Q, K, V all at once\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        # Reshape and split Q, K, V\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.n_heads, self.d_k)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape back\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.output_proj(context)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.attention = QKVAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention with residual connection\n",
    "        attn_output, _ = self.attention(self.norm1(x), mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(self.norm2(x))\n",
    "        x = x + ff_output\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MatrixTransformerProcessor:\n",
    "    def __init__(self, input_dim, n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        \"\"\"Initialize the processor with transformer components\"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Create transformer components\n",
    "        self.transformer = TransformerBlock(\n",
    "            d_model=input_dim,\n",
    "            n_heads=n_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Move model to GPU if available\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.transformer.to(self.device)\n",
    "        \n",
    "    def cublas_matmul(self, A, B):\n",
    "        \"\"\"Matrix multiplication using cuBLAS\"\"\"\n",
    "        if not has_gpu:\n",
    "            return np.matmul(A, B)  # Fallback to numpy\n",
    "            \n",
    "        # Use CuPy for GPU matrix multiplication\n",
    "        A_gpu = cp.asarray(A)\n",
    "        B_gpu = cp.asarray(B)\n",
    "        C_gpu = cp.matmul(A_gpu, B_gpu)\n",
    "        \n",
    "        return cp.asnumpy(C_gpu)\n",
    "    \n",
    "    def process(self, A, B):\n",
    "        \"\"\"Complete workflow: matrix multiply then transform\"\"\"\n",
    "        # Step 1: Perform matrix multiplication\n",
    "        C = self.cublas_matmul(A, B)\n",
    "        \n",
    "        # Step 2: Convert to PyTorch tensor and prepare for transformer\n",
    "        # Transformer expects [batch_size, seq_len, features]\n",
    "        # We'll treat each row as a sequence element\n",
    "        C_tensor = torch.FloatTensor(C).unsqueeze(0)  # Add batch dimension\n",
    "        C_tensor = C_tensor.to(self.device)\n",
    "        \n",
    "        # Step 3: Process with transformer\n",
    "        with torch.no_grad():  # No need for gradients in inference\n",
    "            transformed = self.transformer(C_tensor)\n",
    "        \n",
    "        # Step 4: Convert back to numpy for further processing if needed\n",
    "        result = transformed.cpu().numpy().squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Complete usage example\n",
    "def matrix_transformer_demo():\n",
    "    # Generate matrices\n",
    "    n = 64  # Size should be a multiple of n_heads for optimal transformer usage\n",
    "    A, B = generate_matrices(n)\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = MatrixTransformerProcessor(input_dim=n)\n",
    "    \n",
    "    # Process matrices\n",
    "    result = processor.process(A, B)\n",
    "    \n",
    "    # Compare with standard matrix multiplication\n",
    "    standard_result = np.matmul(A, B)\n",
    "    \n",
    "    # Print shapes\n",
    "    print(f\"Input matrices: {A.shape} x {B.shape}\")\n",
    "    print(f\"Standard result shape: {standard_result.shape}\")\n",
    "    print(f\"Transformed result shape: {result.shape}\")\n",
    "    \n",
    "    # Compare first few values\n",
    "    print(\"\\nFirst few values of standard result:\")\n",
    "    print(standard_result[:3, :3])\n",
    "    print(\"\\nFirst few values of transformed result:\")\n",
    "    print(result[:3, :3])\n",
    "    \n",
    "    return standard_result, result\n",
    "\n",
    "# Helper function to generate matrices (from your original code)\n",
    "def generate_matrices(n, dtype=np.float32):\n",
    "    \"\"\"Generate two random matrices of size n x n\"\"\"\n",
    "    A = np.random.random((n, n)).astype(dtype)\n",
    "    B = np.random.random((n, n)).astype(dtype)\n",
    "    return A, B\n",
    "\n",
    "# Run the demo if GPU is available\n",
    "if has_gpu:\n",
    "    print(\"\\nRunning matrix multiplication with transformer processing demo...\")\n",
    "    standard_result, transformed_result = matrix_transformer_demo()\n",
    "else:\n",
    "    print(\"\\nCUDA device not detected. Demo requires GPU support.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
